from typing import Dict, Any, List
from ..base import BasePipeline

class DatabricksWorkflowsJobAdapter:
    """
    Creates single-task Databricks job from pipeline config
    """
    def generate_deployment(self, pipeline: BasePipeline, output_path: str = None) -> str:
        # Generate JSON for Databricks Job
        job_config = {
            "name": pipeline.config.pipeline_name,
            "tasks": [
                {
                    "task_key": "main_task",
                    "spark_python_task": {
                        "python_file": "main.py",
                        "parameters": [f"--pipeline={pipeline.config.pipeline_name}"]
                    }
                }
            ]
        }
        return str(job_config)

class DatabricksWorkflowsMultiTaskAdapter:
    """
    Builds multi-task DAG with dependencies in Databricks Workflows
    """
    def generate_deployment(self, pipeline: BasePipeline, output_path: str = None) -> str:
        tasks = []
        for task in pipeline.tasks:
            tasks.append({
                "task_key": task.task_name,
                "spark_python_task": {
                    "python_file": "main.py",
                    "parameters": [f"--task={task.task_name}"]
                }
            })
        
        job_config = {
            "name": pipeline.config.pipeline_name,
            "tasks": tasks
        }
        return str(job_config)

class DatabricksWorkflowsDeltaLiveTablesAdapter:
    """
    Generates DLT pipeline.py from config (expectations, target tables)
    """
    def generate_deployment(self, pipeline: BasePipeline, output_path: str = None) -> str:
        dlt_code = f"""
import dlt
from pyspark.sql.functions import *

@dlt.table(
    name="{pipeline.config.destination_table}",
    comment="Generated by Sparkle"
)
def {pipeline.config.destination_table}():
    return dlt.read("{pipeline.config.source_table}")
"""
        return dlt_code

class DatabricksWorkflowsNotebookTaskAdapter:
    """
    Wraps any notebook path as task with parameters from config
    """
    def generate_deployment(self, pipeline: BasePipeline, output_path: str = None) -> str:
        return "notebook_task_json"

class DatabricksWorkflowsRepairRunAdapter:
    """
    Adds repair-run capability to any failed pipeline
    """
    def generate_deployment(self, pipeline: BasePipeline, output_path: str = None) -> str:
        return "repair_run_config"
